---
title: "Mini-Project 2: Adventure 1"
author: ghh
output: 
  html_document:
    toc: true
    toc_float: true
    code_folding: hide
---


```{r}
library(stringr)
library(dplyr)
library(janeaustenr)
library(ngram)
library(stringr) #for regex
library(tidyverse)
library(tidytext)
library(glue)
library(ggplot2)
library(caret)

library(randomForest)
library(gridExtra)
library(rpart)  
library(infer) 
RNGversion("3.6.1")

```




### Check the Data

```{r}
#raw <- read.csv("https://www.macalester.edu/~ajohns24/data/buzzfeed.csv")

raw <- read.csv("https://www.macalester.edu/~ajohns24/data/buzzfeed.csv", header = T, na.strings = c("", " ", NA))

```

```{r warning = FALSE}
do.call(rbind, lapply(levels(raw$source), FUN = function(x){
   tt <- subset(raw, source == x)
   
   result <- table(tt$type)
   result$source <- x
   
   return (result)
 }))
```

```{r warning = FALSE}
do.call(rbind, lapply(levels(raw$authors), FUN = function(x){
   tt <- subset(raw, authors == x)
   
   result <- table(tt$type)
   result$authors <- x
   
   return (result)
 }))
```


## Part 1: Process the data

We summarized the following predictors to describe our data set: # of ! in title, # of ? in title, # of Upper class Words in title, # of quotation marks in text, the length of title, and the length of text.

```{r}
TitleNumOfExc <- str_count(raw$title, "!")
TitleNumOfQue <- str_count(raw$title, "\\?")
TitleNumOfCap <- str_count(raw$title, "\\b[A-Z]{2,}\\b")
TextNumOfQuotation <- str_count(raw$text, "\"")

TitleLength <- str_count(raw$title, "\\W+")
TextLength <- str_count(raw$text, "\\W+")
```

```{r}
newsTemp <- raw %>% 
  mutate(TitleNumOfExc) %>% 
  mutate(TitleNumOfQue) %>% 
  mutate(TitleNumOfCap) %>% 
  mutate(TextNumOfQuotation) %>% 
  mutate(TitleLength) %>% 
  mutate(TextLength)
```



```{r}
temp <- tibble(txt = newsTemp$text) %>% 
  mutate(txt = as.character(txt))


tokens <- temp %>%
  mutate(linenumber = row_number()) %>%
  unnest_tokens(word, txt)

# get the sentiment from the first text: 
tokens <- tokens %>%
  group_by(linenumber) %>%
  inner_join(get_sentiments("nrc")) %>% # pull out only sentiment words
  count(sentiment) %>% # count the # of positive & negative words
  spread(sentiment, n, fill = 0) # %>% # made data wide rather than narrow
```

\
\



```{r}
newsTemp <- cbind(data.frame(newsTemp),data.frame(tokens))
```

```{r}
newsTemp <- newsTemp %>% 
  mutate(anger = anger/TextLength)  %>% 
  mutate(anticipation = anticipation/TextLength)  %>% 
  mutate(disgust = disgust/TextLength)  %>% 
  mutate(fear = fear/TextLength)  %>% 
  mutate(joy = joy/TextLength) %>% 
  mutate(negative = negative/TextLength) %>% 
  mutate(positive = positive/TextLength) %>% 
  mutate(sadness = sadness/TextLength) %>% 
  mutate(surprise = surprise/TextLength) %>% 
  mutate(trust = trust/TextLength)
```



```{r}
newsData <- newsTemp %>% 
  select(-title, -text, -url, -linenumber, -authors)
colnames(newsData)
```


## Part 2: Analyze

We depends on 4 algorithms: Logistic regression, KNN, Tree, Forest.
We first use all 17 available predictors on these models.

### Logistic 

```{r warning = FALSE}
# Set the seed
set.seed(253)

# Run the model
logistic_model <- train(
    type ~ .,
    data = newsData,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)

```

```{r}
summary(logistic_model)
```



CV accuracy:

```{r}
logistic_model$results
```


```{r}
set.seed(253)

# We can only make predictions for cases with information on the predictor
predict_data <- na.omit(logistic_model$trainingData)

# CLASSIFY each case in the sydney data set (using a 0.5 probability threshold)
classifications <- predict(logistic_model, newdata = predict_data, type = "raw")

# Construct a confusion matrix
confusionMatrix(data = classifications, 
  reference = predict_data$.outcome, 
  positive = "real")
```




### KNN


```{r warning = FALSE}
#KNN
set.seed(253)

knn_model <- train(
  type ~.,
  data = newsData,
  preProcess = c("center","scale"),
  method = "knn",
  tuneGrid = data.frame(k = c(seq(1, 182, by = 10), 182)),
  trControl = trainControl(method = "cv", number = 5, selectionFunction = "best"),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
plot(knn_model)
```

```{r}
knn_model$bestTune
```



```{r}
mean(knn_model$resample$Accuracy)
```


```{r}
set.seed(253)

# We can only make predictions for cases with information on the predictor
predict_data <- na.omit(knn_model$trainingData)

# CLASSIFY each case in the sydney data set (using a 0.5 probability threshold)
classifications <- predict(knn_model, newdata = predict_data, type = "raw")

# Construct a confusion matrix
confusionMatrix(data = classifications, 
  reference = predict_data$.outcome, 
  positive = "real")
```


### Classification Tree

```{r}
set.seed(253)
# Construct trees
tree_model <- train(
  type ~ .,
  data = newsData,
  method = "rpart",
  tuneGrid = data.frame(cp = seq(0,0.5,length=70)),
  trControl = trainControl(method = "cv", number = 10, selectionFunction = "oneSE"),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
plot(tree_model)
tree_model$bestTune
```

```{r}
set.seed(253)

# We can only make predictions for cases with information on the predictor
predict_data <- na.omit(tree_model$trainingData)

# CLASSIFY each case in the sydney data set (using a 0.5 probability threshold)
classifications <- predict(tree_model, newdata = predict_data, type = "raw")

# Construct a confusion matrix
confusionMatrix(data = classifications, 
  reference = predict_data$.outcome, 
  positive = "real")
```


```{r}
mean(tree_model$resample$Accuracy)
```

```{r}
# Plot the tree
library(rpart.plot)
rpart.plot(tree_model$finalModel)

# Get metrics of variable importance
tree_model$finalModel$variable.importance
```




### Random Forest

```{r}
set.seed(253)

forest_model <- train(
  type ~ .,
  data = newsData,
  method = "rf",
  tuneGrid = data.frame(mtry = c(2,4, 6, 7, 16, 21, 22,30,35,43)),
  trControl = trainControl(method = "oob"),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
plot(forest_model)
```


```{r}
forest_model$finalModel
```

```{r}
(71+65)/(71+12+26+65)
```



```{r}
forest_model$results
```


```{r}
forest_model$results %>% 
    filter(mtry == forest_model$bestTune$mtry)
```



```{r}
variable_importance <- data.frame(importance(forest_model$finalModel)) %>% 
  mutate(predictor = rownames(.))

# Arrange predictors by importance (most to least)
variable_importance %>% 
  arrange(desc(MeanDecreaseGini)) %>% 
  head(6)

# Arrange predictors by importance (least to most)
variable_importance %>% 
  arrange(MeanDecreaseGini) %>% 
  head(6)
```
  
  
  
  
  
  
  





    Model              in-sample Accuracy       5-fold CV / oob
    ---------------- -----------------------  ------------------
    `logistic_model`       0.9253                0.8158
    `knn`                    ?                      ?
    `tree_model`           0.8103                0.7807
    `forest_model`         0.7816                0.8046



\
\
\
\
\
\



## Part 3: Summarize



\
\
\
\
\
\



## Part 4: Contributions

It's sensible that all articles from CNN website are real news. Therefore, 'Source' as a predictor is kind of cheating and also becomes a limiation for news that are from unknown sources. Therefore, we delete 'Source' and run the 4 algorithms again to see how would the prediction result change. 



### Delete `Source`

```{r}
newsData2 <- newsData %>% 
  select(-source)
```


#### Logistic 

```{r warning=FALSE}
# Set the seed
set.seed(253)

# Run the model
logistic_model_2 <- train(
    type ~ .,
    data = newsData2,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)

```

```{r}
summary(logistic_model_2)
```



CV accuracy:

```{r}
logistic_model_2$results
```


```{r}
set.seed(253)

# We can only make predictions for cases with information on the predictor
predict_data <- na.omit(logistic_model_2$trainingData)

# CLASSIFY each case in the sydney data set (using a 0.5 probability threshold)
classifications <- predict(logistic_model_2, newdata = predict_data, type = "raw")

# Construct a confusion matrix
confusionMatrix(data = classifications, 
  reference = predict_data$.outcome, 
  positive = "real")
```




### KNN

```{r warning = FALSE}
#KNN
set.seed(253)

knn_model_2 <- train(
  type ~.,
  data = newsData2,
  preProcess = c("center","scale"),
  method = "knn",
  tuneGrid = data.frame(k = c(seq(1, 182, by = 10), 182)),
  trControl = trainControl(method = "cv", number = 5, selectionFunction = "best"),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
plot(knn_model_2)
```

```{r}
knn_model_2$bestTune
```



```{r}
mean(knn_model_2$resample$Accuracy)
```


```{r}
set.seed(253)

# We can only make predictions for cases with information on the predictor
predict_data <- na.omit(knn_model_2$trainingData)

# CLASSIFY each case in the sydney data set (using a 0.5 probability threshold)
classifications <- predict(knn_model_2, newdata = predict_data, type = "raw")

# Construct a confusion matrix
confusionMatrix(data = classifications, 
  reference = predict_data$.outcome, 
  positive = "real")
```




### Classification Tree

```{r}
set.seed(253)
# Construct trees
tree_model_2 <- train(
  type ~ .,
  data = newsData2,
  method = "rpart",
  tuneGrid = data.frame(cp = seq(0,0.5,length=70)),
  trControl = trainControl(method = "cv", number = 5, selectionFunction = "oneSE"),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
plot(tree_model_2)
tree_model_2$bestTune
```

```{r}
set.seed(253)

# We can only make predictions for cases with information on the predictor
predict_data <- na.omit(tree_model_2$trainingData)

# CLASSIFY each case in the sydney data set (using a 0.5 probability threshold)
classifications <- predict(tree_model_2, newdata = predict_data, type = "raw")

# Construct a confusion matrix
confusionMatrix(data = classifications, 
  reference = predict_data$.outcome, 
  positive = "real")
```


```{r}
mean(tree_model_2$resample$Accuracy)
```

```{r}
# Plot the tree
library(rpart.plot)
rpart.plot(tree_model_2$finalModel)

# Get metrics of variable importance
tree_model_2$finalModel$variable.importance
```





### Random Forest

```{r}
set.seed(253)

forest_model_2 <- train(
  type ~ .,
  data = newsData2,
  method = "rf",
  tuneGrid = data.frame(mtry = c(2,4, 8, 16)),
  trControl = trainControl(method = "oob"),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
plot(forest_model_2)
```


```{r}
forest_model_2$finalModel
```

```{r}
(70+61)/(70+21+30+61)
```



```{r}
forest_model_2$results
```


```{r}
forest_model_2$results %>% 
    filter(mtry == forest_model_2$bestTune$mtry)
```



```{r}
variable_importance <- data.frame(importance(forest_model_2$finalModel)) %>% 
  mutate(predictor = rownames(.))

# Arrange predictors by importance (most to least)
variable_importance %>% 
  arrange(desc(MeanDecreaseGini)) %>% 
  head(6)

# Arrange predictors by importance (least to most)
variable_importance %>% 
  arrange(MeanDecreaseGini) %>% 
  head(6)
```



    Model                    in-sample Accuracy       5-fold CV / oob
    --------------------  -----------------------  ------------------
    `logistic_model_2`            0.7582                0.7249
    `knn_model_2`                 0.6758                0.6972
    `tree_model_2`                0.7308                0.7141
    `forest_model_2`              0.7198                0.7198
    
    
    
    
### Rebuild Logistic and KNN: Less Variables

For Logistic and KNN, we decide to rerun them with selected predictors, which are the most important predictors identified by the Forest.

#### Logistic 

```{r warning=FALSE}
# Set the seed
set.seed(253)

# Run the model
logistic_model_3 <- train(
    type ~ TextNumOfQuotation + TitleLength + disgust + surprise + positive + TextLength ,
    data = newsData2,
    method = "glm",
    family = "binomial",
    trControl = trainControl(method = "cv", number = 5),
    metric = "Accuracy",
    na.action = na.omit
)

```

```{r}
summary(logistic_model_3)
```



CV accuracy:

```{r}
logistic_model_3$results
```


```{r}
set.seed(253)

# We can only make predictions for cases with information on the predictor
predict_data <- na.omit(logistic_model_3$trainingData)

# CLASSIFY each case in the sydney data set (using a 0.5 probability threshold)
classifications <- predict(logistic_model_3, newdata = predict_data, type = "raw")

# Construct a confusion matrix
confusionMatrix(data = classifications, 
  reference = predict_data$.outcome, 
  positive = "real")
```




### KNN

```{r warning = FALSE}
#KNN
set.seed(253)

knn_model_3 <- train(
  type ~ TextNumOfQuotation + TitleLength + disgust + surprise + positive + TextLength,
  data = newsData2,
  preProcess = c("center","scale"),
  method = "knn",
  tuneGrid = data.frame(k = c(seq(1, 182, by = 10), 182)),
  trControl = trainControl(method = "cv", number = 5, selectionFunction = "best"),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
plot(knn_model_3)
```

```{r}
knn_model_3$bestTune
```



```{r}
mean(knn_model_3$resample$Accuracy)
```


```{r}
set.seed(253)

# We can only make predictions for cases with information on the predictor
predict_data <- na.omit(knn_model_3$trainingData)

# CLASSIFY each case in the sydney data set (using a 0.5 probability threshold)
classifications <- predict(knn_model_3, newdata = predict_data, type = "raw")

# Construct a confusion matrix
confusionMatrix(data = classifications, 
  reference = predict_data$.outcome, 
  positive = "real")
```    

    

### KNN

```{r warning = FALSE}
#KNN
set.seed(253)

knn_model_4 <- train(
  type ~ TextNumOfQuotation + TitleLength,
  data = newsData2,
  preProcess = c("center","scale"),
  method = "knn",
  tuneGrid = data.frame(k = c(seq(1, 182, by = 10), 182)),
  trControl = trainControl(method = "cv", number = 5, selectionFunction = "best"),
  metric = "Accuracy",
  na.action = na.omit
)
```

```{r}
plot(knn_model_4)
```

```{r}
knn_model_4$bestTune
```



```{r}
mean(knn_model_4$resample$Accuracy)
```


```{r}
set.seed(253)

# We can only make predictions for cases with information on the predictor
predict_data <- na.omit(knn_model_4$trainingData)

# CLASSIFY each case in the sydney data set (using a 0.5 probability threshold)
classifications <- predict(knn_model_4, newdata = predict_data, type = "raw")

# Construct a confusion matrix
confusionMatrix(data = classifications, 
  reference = predict_data$.outcome, 
  positive = "real")
```   
  
    

    Model                    in-sample Accuracy       5-fold CV 
    --------------------  -----------------------  ------------------
    `logistic_model_3`            0.7553                0.6922
    `knn_model_3`                 0.6868                0.7143
    `knn_model_4`                 0.7363                0.6866

    
    
    
Summary:


    
      Model                   in-sample Accuracy     5-fold CV / oob
    --------------------  -----------------------  ------------------
    `logistic_model`              0.9253                0.8158
    `knn`                             ?                   ?
    `tree_model`                  0.8103                0.7807
    `forest_model`                0.7816                0.8046
    `logistic_model_2`            0.7582                0.7249
    `knn_model_2`                 0.6758                0.6972
    `tree_model_2`                0.7308                0.7141
    `forest_model_2`              0.7198                0.7198
    `logistic_model_3`            0.7553                0.6922
    `knn_model_3`                 0.6868                0.7143
    `knn_model_4`                 0.7363                0.6866


Highlight the predictors which are most and least useful in classifying news;

Enhance your discussion with visualizations.

Comparing the accuracy of the first 4 models with the second camp where 'Source' is not used, the accuracy drasctially drops from (0.78 - 0.92) to (0.68-0.76). After using selected predictors on logistic and knn(without 'Source'), we have a range of (0.69 - 0.76).

### The most important predictors

       MeanDecreaseGini  Most Important predictors    belongs to Source'
    -------------------  -----------------------     ------------------
	  `20.344026`          	`TextNumOfQuotation`		
	  `12.490735`	        `sourcehttp://politi.co`		    `*`
	  `5.098904`	          `surprise`		
	  `3.897069`	            `joy`		
	  `3.840938`           	`disgust`		
	  `3.657723`         	`sourcehttp://cnn.it`            `*`  
    `7.012116`	           `TitleLength`
	  `6.029909`           	`positive`	
   	`5.833852`	          `TextLength`
   	
Among those, `TextNumOfQuotation`, `surprise`,and `disgust`	are identified as the most important predictors both in 'Source' forest model and the 'Non-Source' forst model. Therefore, we visualize the relationship between them and the response y(type in this case).

####TextNumOfQuotation

```{r}
ggplot(newsData, aes(x = TextNumOfQuotation,fill=type)) + 
  geom_density(alpha=0.5) 
```
real news tend to have large number(0 - 50) of quotations in text, while the number of quotation marks in fake news range from (0 -10) and the most of the fake news have 0 quotations. 

####surprise

```{r}
ggplot(newsData, aes(x = surprise,fill=type)) + 
  geom_density(alpha=0.5) 
```
In general, real news appear to have a normal distribution of (the percentage of) words associated with 'suprise', while fake news tend to have less 'suprise' words. As the density of the two are largely overlaped, it appears that 'surprise' is a much weaker predictor.

####disgust

```{r}
ggplot(newsData, aes(x = disgust,fill=type)) + 
  geom_density(alpha=0.5) 
```
We observe that fake news have a larger portion of text that are associated with 'disgust' than real news do, while real news is twice more likely to have no 'disgust' words than fake news do.

### The least important predictors

       MeanIncreaseGini  Most Important predictors    belongs to 'Source'
    -------------------  -----------------------     ------------------
        0.3159949         	TitleNumOfQue		
        3.6013215         	TitleNumOfExc		
        4.2843891	            anger		
        4.5685442	            fear		
        4.7367663	          TitleNumOfCap		
        4.8588023	          anticipation
                              a lot of source links         *
